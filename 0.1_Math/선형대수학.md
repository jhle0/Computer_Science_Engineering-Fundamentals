# 선형대수학

---

## 1. 벡터 기본

## 1.1 벡터?

벡터는 여러 설명으로 표현된다

- 물리학적 관점 : 공간 상의 화살표
- 컴퓨터 과학 관점 : 숫자의 나열 (배열)

→ **벡터는 “공간에서의 방향과 크기”를 표현하는 객체이며, 좌표로 표현할 수 있다.**

## 1.2 벡터의 2가지 기본 연산

- **벡터 합**
    - 의미: 두 화살표를 “이어 붙이는” 결과
    - 계산: 좌표끼리 더함
- **스칼라 곱(스케일링)**
    - 의미: 길이 늘리기/줄이기, 음수면 방향 뒤집기
    - 계산: 각 좌표에 스칼라를 곱함

→ 이 두 가지 연산이 **선형대수학의 모든 주제의 중심**

## 1.3 기저 벡터, 선형결합, 생성

$\begin{bmatrix}3 \\1\end{bmatrix}$이라는 벡터의 **각 좌표값을 스칼라**로 생각해보면

데카르트 좌표계의 단위벡터인 $\hat{i}$ 과 $\hat{j}$의 스케일링된 값의 합으로 볼수있다

→ 즉, 좌표 쌍이 나타내는 벡터 = 두 스케일링된 벡터의 값

### 선형결합(linear combination)

두 벡터를 스케일링하고 더하여 새 벡터를 얻는 모든 연산

### 생성(Span)

주어진 벡터 집합으로 만들 수 있는 모든 선형결합의 집합

### 기저(Basis)

좌표쌍 벡터에서 각 스칼라(각 좌표값)들이 **스케일 하는 실제 대상**

즉, 벡터 공간의 **기저**는 **공간 전체를 생성하는 선형 독립인 벡터의 집합**

 $\hat{i}$ 과 $\hat{j}$ 은 x,y 좌표계상의 기저(basis)라고 한다

- $\hat i$ - x 축에 있는 단위 벡터
- $\hat j$ - y 축에 있는 단위 벡터

### 선형 독립, 선형 종속

대부분의 두 벡터쌍의 경우 → 평면 위 모든 점에 다다를 수 있음

즉 **span이 무한대**이다

> span - 두 벡터를 통해 표현할 수 있는 공간
> 

⇒ 이게 **선형 독립**

두 기저 벡터가 만약 **같은 직선상**이다 → 벡터의 끝점은 항상 한 직선 위이다

⇒ 이게 **선형 종속**

선형 종속이란 즉 벡터 중 하나가 다른 벡터들의 선형결합으로 표현될 때

> 3차원
> 
> 
> 3차원에서 만약 하나의 벡터가 나머지 두 벡터의 선형결합으로 표현될수 있다면 → 이 세 벡터의 결합은 한 **평면 상에있다**
> 

## 1.4 직교(orthogonality)

- **한 줄 요약:** 직교는 “서로 영향을 안 주는 방향”이며 내적 0으로 판정한다.
- **정의(수식/문장):**
    - $x\perp y \iff x^Ty = 0$
- **직관(그림 없이 설명):**
    - 한 벡터를 다른 벡터 방향으로 정사영했을 때 길이가 0이면(성분이 없으면) 직교다.
- **핵심 성질/공식(최대 3개):**
    1. 직교이면 피타고라스: $\|x+y\|^2=\|x\|^2+\|y\|^2$
    2. 직교 기저에서 좌표 분해가 쉬움
    3. 잔차(residual)는 목표 공간에 직교(least squares 연결)

---

## 2. 내적(dot product)

## 2.1 내적 정의와 계산

벡터의 내적 계산은 간단하다

$\begin{bmatrix}4\\1\end{bmatrix} \cdot \begin{bmatrix}2\\-1\end{bmatrix} =  4 \times2 + 1\times(-1) = 7$

각 좌표끼리 곱하고 더하면 된다

위 식은 $[4 \space\space\space  1]\begin{bmatrix}2\\-1\end{bmatrix}$ 이므로

→ $a \cdot b = a^{T}b(=b^{T}a)$

기하적으로 보면, 

두 벡터 a, b에 대해 내적은 다음과 같이 해석할 수 있다.

- **“a를 b 방향으로 투영한 길이(스칼라 성분)” × “b의 길이”**
- 또는 반대로 **“b를 a 방향으로 투영한 길이” × “a의 길이”**

내적은 아래 식으로 정의된다 :

$a^{T}b = ||a||\cdot ||b|| cos\theta$

- $||a||cos\theta$ 가 a를 b 방향으로 정사영 내린 크기이고
- $||b||cos\theta$ 가 b를 a 방향으로 정사영 내린 크기이다

→ 즉, 내적은 **정사영(projection)** 이다

내적이 의미 하는 것은 두 벡터의 **닮은 정도** 이다

- 두 벡터가 대체로 같은 방향 → 내적 > 0
- 두 벡터가 수직 → 내적 = 0
- 두 벡터가 대체로 반대 방향 → 내적 < 0

## 2.2 정사영된 벡터 구하기

벡터 a를 b의 정사영 했을 때 벡터를 구해보자

정사영된 크기는 $||a||cos\theta$ 이다

이 값은 $a^{T}b$를 $||b||$로 나누면 된다

정사영된 크기는 $\frac{a^Tb}{\sqrt {b^Tb}}$ 이다

이 값에 b 방향을 곱해주면 된다

b 방향 : $\frac{b}{\|b^Tb\|}$

⇒ 즉, 정사영 내린 벡터는 $\frac{a^Tb}{\sqrt {b^Tb}}b$ 이다

### **다른 방법**

b에 정사영 내린 벡터는 b 벡터에 스칼라 곲을 한 값이다

스칼라 값을 $\hat x$ 라고 하자 → 정사영 내린 벡터는 $b\cdot \hat x$

그다음 $a$ 벡터에서 $b\cdot \hat x$ 를 빼 보면

$a - b\cdot \hat x$ 는 $b\cdot \hat x$ 와 수직하게 된다

따라서 이 둘을 내적하면 = 0 이라는 식으로 풀어 $\hat x$ 를 구해서 정사영된 벡터를 구할 수 있다

→ 이 방법은 least squares 유도하는데 사용됨

> <참고>
> 
> 
> $a^{T}b = ||a||\cdot ||b|| cos\theta$ 이다
> 
> 그럼, $a^{T}a = ||a||\cdot ||a|| cos\theta =  ||a||^2$이다
> 
> 그래서 $||a|| = \sqrt{a^Ta}$ 라고 할 수 있다
> 
> 단위 벡터는 크기가 1인 벡터이다(방향성만 가진)
> 
> 단위 벡터를 구하는 방법은 벡터를 벡터의 크기로 나눠주면 된다
> 
> - normalize : $\frac{a}{\sqrt {a^Ta}}$

## 2.3 코사인 유사도(cosine similarity)

- **한 줄 요약:** 크기 제거하고 방향만 비교한다.
- **정의:**
    - $\cos\theta=\frac{x^Ty}{\|x\|\|y\|}$
- **직관:**
    - 길이가 큰 벡터가 유리해지는 문제를 제거하고, 방향 일치 정도만 본다.
- 범위
    - -1부터 1사이의 값이다
    - 1에 가까울수록 두 벡터의 방향이 일치하고 -1이면 정반대 방향
    - 0이면 직교

---

## 3. norm과 거리(norm & distance)

## 3.1 norm의 정의와 종류

norm 은 “길이”의 일반화이다

벡터의 크기를 구해보자

a 벡터가 $\begin{bmatrix}1 \\2 \\3\end{bmatrix}$일 때, $||a|| = \sqrt{1^2 + 2^2 + 3^2}$ 로 구한다

이 것이 2-norm 이다

### 2-norm($l_2$ norm, euclidean norm)

위 처럼 $||a||_2 = \sqrt{1^2 + 2^2 + 3^2}$ 가 바로 2-norm 이다

두 벡터 사이의 distance를 구해보면

두 벡터 $\begin{bmatrix}1 \\2\end{bmatrix}$와 $\begin{bmatrix}2 \\1\end{bmatrix}$의 거리는 $d = \sqrt{(1-2)^2 + (2-1)^2}$ 이다

### 1-norm ($l_1$ norm, manhatten norm)

1-norm 은

a 벡터가 $\begin{bmatrix}1 \\2 \\3\end{bmatrix}$일 때, $||a||_1 = |1| + |2| + |3|$ 이다

### p-norm

위를 보면 각 원소의 p 승을 p분의 1로 나눈걸로 정규화 시킬 수 있다

→  $||x||_p = \sum_i (|x_i|^p)^\frac{1}{p}$

### infinite-norm($\infty$-norm)

p 를 무한대로 보내면

가장 영향력이 큰 값 빼고는 다 무시된다 

$||x||_\infty = {max}|x_i|$

## 3.2 L2/L1/L $\infty$ 등고선 직관

- 각 norm 이 크기가 1인 점들을 좌표평면에 찍어보면
    - L2 : 원
    - L1 : 마름모
    - L $\infty$ : 정사각형

---

## 4. 행렬 기본

## 4.1 선형 변환과 행렬

### 선형 변환

변환은 함수 $T$이다

$T:\mathbb{R}^n \to \mathbb{R}^m$

**변환 = 함수**이다(입력이 들어가면 출력을 내놓는 구조)

즉, 어떤 벡터를 넣으면 다른 벡터가 나옴

이 변환을 특수한 변환 **‘선형’으로만** 변환해서 선형 변환

**선형(linear)** 이 되려면 아래 두 성질을 만족해야 한다.

1. **덧셈 보존**

$T(u+v)=T(u)+T(v)$

1. **스칼라곱 보존**

$T(cu)=cT(u)$

**선형 변환의 2가지 성질**

- 직선은 직선인 채로 유지
- 원점 고정 유지

→ 선형 변환은 공간이 움직이는 방식(격자선이 평행하고 균등한채로)

### 선형 변환과 행렬

벡터 $v$ 가 $\begin{bmatrix}-1 \\2\end{bmatrix} = -1 \hat i + 2\hat j$이고, 기저벡터 $\begin{bmatrix}1 \\0\end{bmatrix}$ $\hat{i}$ 와 $\begin{bmatrix}0 \\1\end{bmatrix}$ $\hat{j}$ 가 있다.

이때 **기저벡터를 변환(선형 변환)**시키면 → **벡터도 변함**

**즉, 기저벡터가 움직이는 도달점만 알면 변환 벡터 값**을 알 수 있다

변환된 $\hat i$ 가 $\begin{bmatrix}1 \\-2\end{bmatrix}$이고, 변환된 $\hat j$ 가 $\begin{bmatrix}3 \\0\end{bmatrix}$이면

$new \space v = -1(new \space \hat i) + 2(new \space \hat j)$

$new \space v = -1\begin{bmatrix}1 \\-2\end{bmatrix} + 2\begin{bmatrix}3 \\0\end{bmatrix} = \begin{bmatrix}1 \space\space3 \\-2 \space 0\end{bmatrix}\begin{bmatrix}-1 \\2\end{bmatrix} =\begin{bmatrix}5 \\2\end{bmatrix}$

즉, 선형 변환을 행렬 $\begin{bmatrix}1&3 \\-2 &0\end{bmatrix}$로 기술할 수 있다

> 행렬 $\begin{bmatrix}a& b \\c & d\end{bmatrix}$에서
> 
> 
> 1열의 $\begin{bmatrix}a \\c\end{bmatrix}$는 첫번째 기저 벡터가 도달하는 좌표로 해석
> 
> 2열의 $\begin{bmatrix}b \\d\end{bmatrix}$는 두번째 기저 벡터가 도달하는 좌표로 해석할 수 있다
> 
> 즉, 행렬의 **각 열(column)** 은 **기저벡터가 어디로 가는지**를 담고 있다.
> 

### ⇒ **핵심 개념 : 선형 변환은 행렬로 기술된다**

## 4.2 행렬 연산과 shape 감각

### (1) $x^Ty$ : **내적(dot product)**

- **shape**
    - $x\in\mathbb{R}^{n\times1}$, $y\in\mathbb{R}^{n\times1}$
    - $x^T\in\mathbb{R}^{1\times n}$
    - $x^Ty\in\mathbb{R}^{1\times1}$ → **스칼라**
- **의미**
    - “두 벡터의 유사도/정렬 정도”
    - 기하적으로 $\|x\|\|y\|\cos\theta$

### (2) $xy^T$ : **외적(outer product)**

- **shape**
    - $x\in\mathbb{R}^{n\times1}$, $y^T\in\mathbb{R}^{1\times n}$
    - $xy^T\in\mathbb{R}^{n\times n}$ → **행렬**
- **의미**
    - “ $x$  방향 성분을 $y$ 성분으로 스케일해서 만든 **랭크 1 행렬**”
    - 모든 열이 $x$의 스칼라배, 모든 행이 $y^T$ 의 스칼라배

## 4.3 선형변환의 합성과 행렬의 곱셈

두 선형 변환을 한 경우를 보자

벡터 $v = \begin{bmatrix}x\\y\end{bmatrix}$에 **회전(rotation) 후 전단(shear)** 을 적용해보자

회전 : $R = \begin{bmatrix}0 & -1 \\1 &  0\end{bmatrix}$

전단 : $S = \begin{bmatrix}1 & 1 \\0 &  1\end{bmatrix}$

그러면 합성 변환은 $R(Sv) = (RS)v$ 이며, **오른쪽 행렬부터 먼저 적용**된다

### 합성 행렬 계산

 $RS = \begin{bmatrix}1 & 1 \\0 &  1\end{bmatrix}\begin{bmatrix}0 & -1 \\1 &  0\end{bmatrix}$ 이다 (변환의 적용은 오른쪽에서 왼쪽으로 적용된다)

기저벡터 관점으로 따라가보면 회전하고 전단을 진행했을 때 $\begin{bmatrix}1 & -1 \\1 &  0\end{bmatrix}$가 된다

결국, $\begin{bmatrix}0 & -1 \\1 &  0\end{bmatrix}\begin{bmatrix}1 & 1 \\0 &  1\end{bmatrix} = \begin{bmatrix}1 & -1 \\1 &  0\end{bmatrix}$이라는 결과가 나온다

**두 행렬의 곱셈**이 **기하학적**으로 나타내는 것은 **한 변환과 다른 변환의 순차적 적용**이다

즉, **행렬 곱** $RS$ 는 **“회전 후 전단”이라는 선형변환의 합성**을 나타내며, 곱의 결과가 곧 **합성 행렬**이다

### 일반형 정리(열 벡터 관점)

$\begin{bmatrix}a & b \\c &  d\end{bmatrix}\begin{bmatrix}e & f \\g &  h\end{bmatrix}$로 다시 한번 보면

먼저 $\hat i$ 의 좌표는 첫번째 선형 변환에 의해 $\begin{bmatrix}e\\g\end{bmatrix}$가 된다 

→ 그리고 이 벡터 $\begin{bmatrix}e\\g\end{bmatrix}$ 에 $\begin{bmatrix}a & b \\c &  d\end{bmatrix}$선형 변환을 적용시키면

$e\begin{bmatrix}a\\c\end{bmatrix}+ g\begin{bmatrix}b\\d\end{bmatrix} = \begin{bmatrix}ae + bg \\ce+  dg\end{bmatrix}$   → 이것이 행렬 곱의 **첫번째 열**이다

다음으로 $\hat j$ 의 좌표는 첫번째 선형 변환에 의해 $\begin{bmatrix}f\\h\end{bmatrix}$가 된다

→ 그 다음 이 벡터 $\begin{bmatrix}f\\h\end{bmatrix}$에  $\begin{bmatrix}a & b \\c &  d\end{bmatrix}$선형 변환을 적용시키면

$f\begin{bmatrix}a\\c\end{bmatrix}+ h\begin{bmatrix}b\\d\end{bmatrix} = \begin{bmatrix}af + bh \\cf+  dh\end{bmatrix}$ → 이것이 행렬 곱의 **두번째 열**이다

⇒ $\begin{bmatrix}a & b \\c &  d\end{bmatrix}\begin{bmatrix}e & f \\g &  h\end{bmatrix} = \begin{bmatrix}ae +bg & af + bh \\ce+dg & cf + dh\end{bmatrix}$가 된다

> 두 선형 변환이 $m1, m2$ 일때
> 
> 
> $m1m2 = m2m1$ 일까?
> 
> 답은, 일반적으로 **아니오!**
> 
> 왜냐하면 순서가 바뀌면 **최종 결과가 달라지는 경우가 대부분**이다
> 
> (예외, 항등행렬, 같은 변환..)
> 

## 4.4 행렬식(determinant)

선형변환을 이해하는 또 다른 유용한 방법

→ **공간이 넓어지거나 찌그러지는 정도(스케일)를 실제로 재는 값**이다

기저 벡터 $\hat i ,\hat j$ 이 이루는 넗이는 1 x 1 

이때  $\begin{bmatrix}3 & 0 \\0 &  2\end{bmatrix}$인 선형 변환이 일어나면

$\hat i$은 3배, $\hat j$ 은 2배로 → 넓이가 **2 x 3 인 직사각형**이 된다

즉, 넓이가 1 → 6 으로 바뀌므로

⇒ 이 선형 변환은 **모든 넓이를 6배 스케일**했다고 말할 수 있다

선형변환으로 인해 넓이가 어떤 스케일 인자만큼 변할 때,

그 인자를 해당 변환의 **행렬식(determinant)** 이라 한다

$det(\begin{bmatrix}3 & 0 \\0 &  2\end{bmatrix}) = 6$

**<행렬식 계산>**

$det(\begin{bmatrix}a & b \\c &  d\end{bmatrix}) = ad - bc$

### 행렬식이 음수인 경우

$det(\begin{bmatrix}1 & 2 \\3 &  4\end{bmatrix}) = -2$ 는 무슨 의미 일까?

- **부호(sign)** 는 **방향(orientation)** 과 관련 있다
- 즉, **음수 행렬식은 공간이 뒤집혔다**는 뜻이다
- (절댓값 은 여전히 넓이가 스케일된 정도를 나타낸다)

### 3차원에서는?

3차원에서의 행렬식은 **평행육면체의 부피를 얼마나 스케일하는지**를 의미한다

$det(\begin{bmatrix}a & b & c \\d & e & f  \\ g&h&i\end{bmatrix}) = a*det(\begin{bmatrix}e & f \\h &  i\end{bmatrix})  -b* det(\begin{bmatrix}d & f \\g &  i\end{bmatrix})+ c*det(\begin{bmatrix}d & e \\g &  h\end{bmatrix})$

### 행렬식이 0인 경우

→ 행렬식이 알려주는 가장 중요한 정보 중 하나는 변환이 **선형 종속(차원 축소)** 인지 여부이다

$det = 0$ 이면

- 모든 공간이 더 작은 차원으로 줄어들었다는 뜻이다
    - 2차원 → 1차원
    - 3차원 → 2차원

이런 선형 종속 변환에서는 면적/부피가 **0**이 된다

## 4.5 특수 행렬들

### 정방 행렬(square matrix)

- 행과 열의 수가 같은 행렬
    - 선형 대수학의 많은 연산에 필수적인 행렬이다

### 항등 행렬(identity matrix, $I$)

- 곱했을 때 그대로 인, 항등원의 역할
- 대각선의 값이 1이고 다른 값은 모두 0인 행렬
    - 행렬 $A$에 $A^{-1}$를 곱해주면 항등행렬 $I$가 나온다
        
        $\begin{bmatrix}1 & 0& 0 \\0 & 1 & 0  \\ 0&0&1\end{bmatrix}$
        

### 대각행렬(diagonal matirx, $D$)

- 대각선에만 값이 있고 나머지는 모두 0
    - 벡터 공간에 적용되는 간단한 스칼라 값을 나타냄
- $diag( )$
    - $a= \begin{bmatrix}a1 \\a2  \\ a3\end{bmatrix}, diag(a) =?$
    - $diag(a) = \begin{bmatrix}a1 & 0 & 0 \\0 & a2 & 0  \\ 0&0&a3\end{bmatrix}$
    - $diag(\begin{bmatrix}a1 & a2 & a3 \\a4 & a5 & a6  \\ a7&a&a9\end{bmatrix}) = \begin{bmatrix}a1 \\a5  \\ a9\end{bmatrix}$

### 대칭 행렬(symentric matrix)

- 전치 행렬과 자기 자신이 같은 행렬
- $A = A^T$

### 직교행렬(orthogonal matrix, $Q$)

- 행렬의 모든 column  들이 서로 서로 직교하는 행렬
    - 각 column 들이 orthonormal 하다
        
        $\begin{bmatrix}1 & 0& 0 \\0 & 1 & 0  \\ 0&0&1\end{bmatrix}$
        
- 전치 행렬($A^{T}$)이 곧 역행렬($A^{-1}$)인 행렬을 말한다
    - $Q^{T}Q = I$
    - $Q^{-1} = Q^T$

---

## 5. 연립방정식, 역행렬, 부분공간

## 5.1 연립 선형 방정식의 기하학적 해석

선형대수의 중요한 쓰임 중 하나는 **연립 선형 방정식**을 푸는 것이다

$2x + 3y = -4$ 

$1x + 3y = -1$

$→  \begin{bmatrix}2 & 3 \\1 &  3\end{bmatrix}\begin{bmatrix}x\\y\end{bmatrix} = \begin{bmatrix}-4 \\-1\end{bmatrix}$

위 식의 선형변환 $\begin{bmatrix}2 & 3 \\1 &  3\end{bmatrix}$ 를 $A$, $\begin{bmatrix}x\\y\end{bmatrix}$벡터를 $x$, $\begin{bmatrix}-4 \\-1\end{bmatrix}$벡터를 $v$ 라고 하면

→즉,  $Ax = v$

이 식의 기하학적 해석:

“선형변환 A를 적용했을 때 결과가 v가 되도록 만드는 입력 벡터 x를 찾는 문제”이다

### 1) det(A) ≠ 0 인 경우 (가역 / invertible)

- 변환 A가 공간을 더 낮은 차원으로 **찌그러뜨리지 않는 경우**이다.
- 이때는 **모든 v에 대해 해 x가 정확히 하나** 존재한다.

이때 변환을 “되돌리는” 행렬이 존재하며, 그것이 **역행렬** $A^{-1}$ 이다.

### 역행렬의 성질

역행렬이 성질 에는 행렬과 역행렬을 곱하면

$A^{-1} A = I$

- $I$는 **항등행렬(항등변환)**: “아무것도 하지 않는 변환”이다.

따라서 해는

- $x = A^{-1}v$

### 2) det(A) = 0 인 경우 (비가역 / non-invertible)

- 변환 A가 공간을 **하위 차원으로 압축**하는 경우이다
- 이때는 일반적으로 **역행렬이 존재하지 않는다**
    
    (직선으로 눌린 걸 다시 평면으로 복구할 수 없기 때문)
    

하지만 **det(A)=0이어도 해가 존재할 수는 있다**

핵심:

- 해가 존재 ⇔ $v$가 **A가 만들 수 있는 출력의 집합** 안에 있을 때

이 “가능한 출력의 집합”이 바로 **열공간(Column Space)** 이다

## 5.2 열공간(Column Space), 행공간(row space)

- 정의: $Ax$ 로 만들 수 있는 **모든 출력 벡터들의 집합**
- 동치: $A$의 **열벡터들의 span(생성 공간)**

즉,

- 열공간 = “A가 도달 가능한 공간”
- $Ax=v$ 가 풀리려면 $v$가 열공간 안에 있어야 한다

- 행공간은 열공간과 마찬가지로 **A의 행 벡터**들이 span하는 공간이다

## 5.3 랭크(Rank)

행렬이 가지는 independent 한 column의 수

= column space의 dimension 이다

**independent 한 column의 수 = independent 한 row의 수 이다**

따라서, 랭크 = row space의 dimension 도 된다

$rank(A) = rank(A^T)$

행렬 $\begin{bmatrix}1 & 2 & 3 \\0 &  0 & 0\end{bmatrix}$의 rank 는 1이다

- independent 한 열이 1개이다 → rank = 1

행렬 $\begin{bmatrix}1 & 0 & 1 \\0 &  1 & 1\end{bmatrix}$의 rank = 2

- $\begin{bmatrix}1\\0\end{bmatrix}, \begin{bmatrix}0 \\1\end{bmatrix}$로 $\begin{bmatrix}1 \\1\end{bmatrix}$를 만들수 있으므로
- independent 한 열은 2개 → rank = 2
- 이 행렬은 row 개수만큰 rank 가 꽉차므로 → **full row rank** 라고 한다

위 $\begin{bmatrix}1 & 2 & 3 \\0 &  0 & 0\end{bmatrix}$ 행렬은 row = 2 인데 rank = 1 이므로 → **rank-deficient** 라고 한다

3x2 행렬이 rank = 2 이면 → **full column rank** 이고

3x3 행렬이 rank = 3 이면 → **full rank** 라고 한다

## 5.4 영공간(Null Space)

- 정의: $Ax=0$을 만족하는 **모든 입력 벡터** $x$**의 집합**
- 즉, “A를 적용했더니 원점(0)으로 가버리는 방향들”

### **ex 1)**

행렬 $\begin{bmatrix}1 & 0 & 1 \\0 &  1 & 1\end{bmatrix}$는 $Ax = x_1\begin{bmatrix}1 \\0\end{bmatrix} + x_2\begin{bmatrix}0 \\1\end{bmatrix} + x_3\begin{bmatrix}1 \\1\end{bmatrix} =\begin{bmatrix}0 \\0\end{bmatrix}$ 을 만족하려면

1. $x = \begin{bmatrix}0 \\0 \\0 \end{bmatrix}$이면 된다 → 이 값은 **null space 에 항상 포함**된
2. 아니면, $x$ 가 $\begin{bmatrix}1 \\1 \\-1 \end{bmatrix}. \begin{bmatrix}2 \\2 \\-2 \end{bmatrix}, ...$ 이어도 된다
    
    → $x_n = c\begin{bmatrix}1 \\1 \\-1 \end{bmatrix}$이다 
    

즉, x 의 null space 는 3차원 공간 안에서 1차원을 span 한다

### **ex 2)**

행렬 $\begin{bmatrix}1 & 2 & 3 \\0 &  0 & 0\end{bmatrix}$의 null space 를 찾아보면

$x = \begin{bmatrix}-2 \\1 \\0 \end{bmatrix}$또는 $x = \begin{bmatrix}-3 \\0 \\11 \end{bmatrix}$일 때 이다

이들을 모두 상수배 하고 더하면

$A(c_1x_1) = 0, A(c_2x_2) =0$ 이므로 → $A(c_1x_1 + c_2x_2) =0$ 이다

→ $x_n = c_1\begin{bmatrix}-2 \\1 \\0 \end{bmatrix} + c_2\begin{bmatrix}-3 \\0 \\1 \end{bmatrix}$ 

따라서 null space 의 차원은 $x_1, x_2$가 span 하는 2차원이다

ex1과 ex2 를 보면

ex 1 에서는 행렬의 rank = 2, null space의 dimension = 1

ex 2 에서는 rank = 1,  null space의 dimension = 2 이다

보면, rank 와 null space의 dimension을 합하면 column 의 수가 나온다

### → 즉, A 가 mxn 행렬 일 때, $dim(N(A)) = n - r$ 이다

## 5.5 부분공간의 관계

null space와 row space 는 수직한 space 이다!

$Ax = 0$ 을 보면 A의 row 벡터와 x의 null space 를 곱하면 0이 나와야 한다

즉, A의 row 들이 span 하는 row space 전체와 x의 null space 는 수직해야 한다

$dim(N(A)) = n - r$ 이 식을 보면

r 은 row space의 dimension 으로도 볼 수 있으므로

→ $n = dim(N(A)) + dim(R(A))$ 이다

즉, 실수 n 차원 공간은 row space의 dimension 과 null space의 dimension의 합이다

<left null space>

Ax를 column space 처럼 해석한 것처럼

$x^TA$를 row space 로 바라보면

$x^TA = 0$ 으로 만드는 x 의 집합을 left null space 라고 한다

$dim_L(N(A)) = m - r$ 이고, r 이 column space 의 dimension 이므로

→ $m = dim(N(A^T))+dim(C(A))$ 

![foursubspaces.jpg](attachment:1e54556f-6ac2-4df4-a576-7b084e1f14f6:foursubspaces.jpg)

## 5.6 Rank-Nullity 정리

- **한 줄 요약:** 입력 차원은 “살아남는 차원(rank) + 사라지는 차원(nullity)”로 분해된다
    - $n = nullity(A) + rank(A)$
