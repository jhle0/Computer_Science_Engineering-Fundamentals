# 3.1-6 Memory Management & Virtual Memory

> Active recall
> 
> - Explain the difference between a **logical address** and a **physical address**,
>     
>     and describe how address binding actually splits responsibilities between **CPU, MMU, and OS**.
>     
>     (What does the CPU see, what does the MMU do, and what does the OS configure?)
>     
> - In contiguous allocation, why is **external fragmentation** practically inevitable,
>     
>     and what is the basic idea and limitation of **compaction** as a mitigation technique?
>     
> - Explain the core goal of **virtual memory** from the perspective of
>     
>     “how to make a process larger than physical memory appear executable,”
>     
>     and summarize the basic idea of **paging** as its implementation strategy.
>     
> - In a paging system, describe the roles of the **page table** and the **TLB**.
>     
>     Why does a pure page-table-only design require **two memory accesses per reference**,
>     
>     and how does the TLB reduce this overhead?
>     
> - List the main bits in a **Page Table Entry (PTE)**
>     
>     (valid bit, protection bit, reference bit, modified/dirty bit),
>     
>     explain what each one means, and give examples of **when** each is used in practice.
>     
> - Explain the execution flow of **demand paging** and **page faults** step by step,
>     
>     from the CPU’s perspective → hardware trap → OS handling → returning to user code.
>     
> - For page replacement algorithms **FIFO / Optimal / LRU / LFU / Clock**,
>     
>     write one line for each describing **what criterion** it uses to pick a victim page,
>     
>     and explain with a simple example why LFU fails to capture **recency** properly.
>     
> - Define **thrashing**, explain why it tends to occur
>     
>     when there are not enough frames to hold the **working set**,
>     
>     and compare the high-level ideas behind frame-allocation policies
>     
>     **(equal allocation / proportional allocation / working set / page-fault frequency)**
>     
>     in terms of how they try to reduce thrashing.
>     

---

## Overview of Memory Management

### Logical vs Physical Address, and Address Binding

- **Logical address**
    - The address space **seen by each process independently**.
    - From the process’s point of view, it always looks like a **continuous region starting at 0**.
    - The addresses the **CPU uses when executing instructions** are logical addresses.
- **Physical address**
    - The address in **actual DRAM (physical memory)**.
    - Code, data, and stacks from multiple processes are mixed together in this space.

Converting a logical address → physical address is called **address binding**.

At runtime, the **MMU (Memory Management Unit)** performs this translation:

- The **CPU** generates logical addresses.
- The **MMU** translates logical → physical addresses according to mapping rules.
- The **OS** manages and sets those mapping rules:
    - Page table, segment table, base/limit registers, etc.

### Contiguous vs Noncontiguous Allocation

Depending on how we place a process’s address space in physical memory:

- **Contiguous allocation**
    - Each process is given **one continuous block** of physical memory.
    - Examples:
        - Fixed partitioning
        - Variable partitioning
- **Noncontiguous allocation**
    - A process’s logical address space is split into **multiple pieces**
        
        and can be placed **anywhere** in physical memory.
        
    - Examples:
        - Paging
        - Segmentation
        - Paged segmentation (paging + segmentation)

---

## Contiguous Memory Allocation

Contiguous allocation means each process occupies **one continuous physical memory region**.

### Swapping

Among the processes loaded in memory, some are **not currently using the CPU**.

In that case:

- We can **evict such a process** to a special area in secondary storage (disk)
    
    called the **swap area**, and
    
- Load another process into the freed physical memory.

This is called **swapping**.

Terminology:

- **Swap area**
    - The part of the disk used to store **processes (or pages) evicted from memory**.
- **Swap-out**
    - Moving a process from memory → swap area.
- **Swap-in**
    - Bringing a process back from the swap area → memory.

### Allocation Strategies in Contiguous Allocation

When there are multiple **free blocks (holes)** in memory,

and we want to place a process contiguously, we must choose which hole to use.

Typical strategies:

- **First Fit**
    - Scan the free list from the beginning, and
        
        allocate in the **first hole that is large enough**.
        
    - Pros:
        - Minimizes search.
        - Allocation is relatively fast.
- **Best Fit**
    - Scan **all holes**, then choose the **smallest hole that fits** the process.
    - Goal:
        - Reduce leftover unused space in the chosen hole.
    - Con:
        - Can lead to many **small unusable fragments**.
- **Worst Fit**
    - Scan **all holes**, then choose the **largest** hole.
    - Intuition:
        - After allocation, the leftover piece may still be large enough to be useful.

### External Fragmentation and Compaction

The main problem of contiguous allocation:

> As processes repeatedly start and terminate,
> 
> 
> **small free blocks appear scattered among allocated regions**.
> 
- The total free memory may be sufficient,
- But because it is split into many small pieces,
- No single hole is large enough to hold a new, large process.

This kind of wasted free space is **external fragmentation**.

**Compaction** is a classic mitigation:

- Move allocated blocks in memory to **coalesce free space into one large block**.
- Downsides:
    - Requires moving lots of data in memory → **high overhead**.
    - During compaction, the system may have to **pause or severely limit normal execution**.

Another solution is to avoid contiguous allocation entirely using **virtual memory + paging**:

- Split processes into fixed-size units (**pages**) and place them anywhere in memory.
- This eliminates external fragmentation,
    
    but introduces **internal fragmentation** at the page granularity.
    

---

## Paging and Virtual Memory

### Virtual Memory

Two major issues with simple contiguous allocation:

1. **External fragmentation**.
2. You cannot run a process **larger than physical memory**.

The key idea of **virtual memory**:

> Do not load the entire program into memory at once.
> 
> 
> Instead, load **only the portions that are actually needed**,
> 
> so that even processes larger than physical memory can run.
> 

Typical implementations:

- Paging
- Segmentation
- Paging + segmentation

### What is Paging?

The root cause of external fragmentation:

> Processes of varying sizes are being allocated
> 
> 
> as **contiguous chunks** in physical memory.
> 

Paging changes the model:

- The logical address space of a process is split into equal-sized units called **pages**.
- Physical memory is split into equal-sized units (same size as a page) called **frames**.
- Logical pages can be mapped to **any frames**, so the process can be **scattered** in memory.

Properties:

- From the process’s viewpoint, the address space is still **logically contiguous**.
- In physical memory, its pages are **noncontiguous and scattered**.
- External fragmentation is effectively eliminated.
- Internal fragmentation arises because the last page of a process may not use its frame fully.

> In a demand-paged virtual memory system,
> 
> 
> swapping happens at the **page** level:
> 
> unnecessary pages are paged out to disk,
> 
> and required pages are brought back to memory (page-in).
> 

### Page Table & TLB

When pages are scattered, the system must map:

> Logical page number → physical frame number.
> 

This mapping is stored in the **page table**.

- **Page Table**
    - Maps each logical page number to a physical frame number.
    - Each process has its **own page table**.
    - The page table itself is stored in memory.

Naively, every memory access requires:

1. One memory access to read the page table entry, and
2. One memory access to read/write the actual data.

→ This means **two memory accesses per logical access**, which is costly.

To mitigate this, we use a **TLB (Translation Lookaside Buffer)**:

- A small hardware cache next to the MMU, storing recent **page → frame** translations.
- On each address translation:
    - If the mapping is found in the TLB (TLB hit),
        
        we get the frame number **without going to memory** for the page table.
        
    - On a TLB miss, the system reads the page table from memory and then caches it in the TLB.

### Multilevel Page Tables (Two-level, Multi-level)

With a large logical address space, a single page table can become **very large**.

- Keeping the entire table in memory at all times is wasteful.

Solution: **multilevel page tables**.

- The page table is organized hierarchically:
    - A **top-level page table** points to **second-level page tables**, and so on.
- Only the portions of the page table covering the **actually used address range**
    
    must be resident in memory.
    
- Example: **Two-level page table**
    - Top-level entry holds the base address of a second-level table.
    - Only second-level tables that cover used regions need to exist in memory.

### Shared Pages

When multiple processes use the **same code** (e.g., shared libraries, libc):

- We do not need to load duplicate copies into memory.
- Instead, their page tables can point to the **same physical frames**.
- Benefits:
    - **Code is loaded once** and shared by many processes → significant memory savings.

### Address Translation in Paging

In a paging system, a logical address is generally split into:

> Page number (p) + offset (d)
> 

Translation steps:

1. CPU generates logical address (p, d).
2. MMU looks up page number **p** in the page table and finds frame number **f**.
3. Physical address = **base address of frame f** + offset **d**.

Notes:

- Each page/frame contains many bytes (e.g., 4 KB).
- The offset tells how far from the **start of the frame** the desired byte is.

### Page Table Entry (PTE)

Each row in the page table is a **Page Table Entry (PTE)**.

Besides the frame number, a PTE usually holds several flag bits:

- **Valid bit**
    - Indicates whether this mapping is **currently valid**.
    - Typical meaning:
        - 1: The page is valid and currently mapped to a frame in memory.
        - 0: The page is not in memory (on disk or unused).
    - If the valid bit is 0 and the page is accessed → **page fault**.
- **Protection bits**
    - Control access permissions to the page:
        - Read, write, execute.
    - Example:
        - `100` → read-only
        - `011` → write + execute (just an illustrative encoding)
- **Reference bit (Access bit)**
    - Indicates whether this page has been **accessed recently**.
    - Used by page replacement algorithms (e.g., LRU approximations, Clock algorithm)
        
        to approximate **recency of use**.
        
- **Modified bit (Dirty bit)**
    - Indicates whether this page has been **written to**.
        - 1: The in-memory copy differs from the disk copy.
        - 0: The in-memory copy is identical to disk.
    - When evicting a page:
        - If dirty = 1 → must **write the page back to disk**.
        - If dirty = 0 → can just drop it; no write-back needed.

---

## Page Replacement and Frame Allocation

Virtual memory allows us to run processes larger than physical memory,

but physical memory is still **finite**.

> When memory is full, we must choose which pages to evict,
> 
> 
> and we must decide **how many frames each process gets**.
> 

### Demand Paging and Page Faults

**Demand paging**:

- Do not load all pages of a process up front.
- Only load a page **when it is first accessed**.

Benefits:

- Less I/O.
- Lower memory usage.
- Faster initial response.
- More processes can be kept resident simultaneously.

**Page fault**:

- When the CPU accesses a page with **valid bit = 0**,
    
    the hardware raises a **page fault trap** to the OS.
    
- The OS then:
    1. Finds a free frame (or chooses a victim page using a replacement algorithm).
    2. Reads the required page from disk into that frame.
    3. Updates the page table.
    4. Restarts the instruction that caused the fault.

For this system to behave well, we need:

1. A good **page replacement algorithm**.
2. A sound **frame allocation policy**.

### Page Replacement Algorithms

When memory is full and a new page must be brought in,

we must decide which page to evict.

This is governed by the **page replacement algorithm**.

Generally, an algorithm is considered “good” if it results in **few page faults**.

### (1) FIFO (First-In First-Out)

- Evict the page that has been in memory **the longest**.
- Implementation: queue.
- Problem:
    - A page that was loaded long ago but is still heavily used can be evicted.
    - Exhibits **Belady’s anomaly** (more frames can result in more faults).

### (2) Optimal (OPT)

- Evict the page that will be **referenced farthest in the future**.
- Assumes perfect knowledge of future references.
- Properties:
    - Minimizes page faults in theory → **optimal benchmark**.
    - Not implementable in real systems; used as a comparison baseline.

### (3) LRU (Least Recently Used)

- Evict the page that has **not been used for the longest time**.
- Uses the assumption of locality:
    - Pages used recently are likely to be used again soon.
- Exact implementation requires tracking full recency info → high overhead.

### (4) LFU (Least Frequently Used)

- Evict the page with the **lowest reference count**.
- Tries to reflect **popularity**.
- Drawback:
    - Does not capture **recency**.
    - Example:
        - A page that was heavily used long ago but is no longer used
            
            may have a high count and **never get evicted**,
            
            while newly important pages with low cumulative counts get evicted.
            

→ Because exact LRU/LFU are expensive, systems use **approximations** like Clock.

### (5) Clock Algorithm (Second-Chance)

An efficient approximation of LRU using hardware support:

- Maintain a circular list of frames → “clock”.
- Each frame has a **reference bit**.
- A “clock hand” points to frames in turn:

Algorithm:

1. If the referenced frame’s bit is **0**:
    - It has **not been used recently** → select it as victim.
2. If the bit is **1**:
    - Clear it to 0 (give a **second chance**) and move the hand forward.
3. Repeat until a frame with bit 0 is found.

Effect:

- Pages accessed recently get their bit set to 1 and thus **survive at least one full sweep**.
- Pages that have not been accessed for a while end up with bit 0 when the hand returns → get evicted.

---

## Thrashing and Frame Allocation

**Thrashing**:

- A situation where a process spends more time **handling page faults and paging**
    
    than executing actual instructions.
    
- System symptom:
    - High page fault rate, low effective CPU utilization.

Root cause:

- Each process has **too few frames** to hold its **working set**
    
    (the set of pages it actively uses over a time window).
    

To mitigate thrashing, the OS must choose **how many frames to allocate per process**.

### (1) Equal Allocation

- Give **the same number of frames** to each process.
- Problem:
    - Processes differ drastically in size and locality.
    - Large processes may thrash; small ones may have unused frames.

### (2) Proportional Allocation

- Allocate frames **in proportion to process size** (or priority).
- Example:
    - Total frames: 100
        
        P1: 200 KB, P2: 300 KB, P3: 500 KB
        
        → allocate 20, 30, 50 frames, respectively.
        
- More reasonable than equal allocation because larger processes get more frames.

### (3) Working Set Model

- Idea:
    - At any given time, a process actively uses a certain set of pages → **working set**.
    - If we ensure the working set is fully in memory, page faults stay low.
- OS behavior:
    - Track recent page references to estimate working set size.
    - Allocate frames according to the working set size.
    - If the sum of all processes’ working sets exceeds physical memory:
        - **Swap out** some processes completely to free enough frames.
- Goal:
    - Avoid entering thrashing by ensuring **each active process has enough frames** for its working set.

### (4) Page-Fault Frequency (PFF)

- Monitor **page-fault rate** per process.
- If a process’s page-fault rate is:
    - **Too high** → allocate **more frames** to that process.
    - **Too low** → possibly **take back some frames** to give to others.
- Objective:
    - Keep each process’s page-fault rate within a **target band**
        
        by dynamically adjusting its frame allocation.
