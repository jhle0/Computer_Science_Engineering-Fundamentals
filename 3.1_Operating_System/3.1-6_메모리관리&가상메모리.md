# 3.1-6 메모리 관리 & 가상 메모리

> Active recall
> 
> - **논리 주소(logical address)** 와 **물리 주소(physical address)** 의 차이를 설명하고,
>     
>     주소 바인딩이 실제로는 **CPU–MMU–OS** 사이에서 어떻게 역할이 나뉘어 있는지 말해봐라.
>     
> - 연속 메모리 할당(contiguous allocation)에서 **외부 단편화(external fragmentation)** 가
>     
>     왜 필연적으로 생기는지,
>     
>     그리고 이를 줄이기 위해 쓸 수 있는 방법인 **압축(compaction)** 의 원리와 한계를 설명해봐라.
>     
> - **가상 메모리(Virtual Memory)** 의 핵심 목적을
>     
>     “물리 메모리보다 큰 프로세스를 어떻게 실행 가능한 것처럼 보이게 만드는지” 관점에서 설명하고,
>     
>     그 구현 방식인 **페이징(paging)** 의 기본 아이디어를 한 번에 정리해봐라.
>     
> - 페이징 시스템에서 **페이지 테이블(Page Table)** 과 **TLB** 의 역할을 각각 설명하고,
>     
>     “왜 순수 페이지 테이블만으로는 매 접근마다 메모리 2번을 가야 하는지”,
>     
>     그리고 TLB가 이를 어떻게 줄여주는지 흐름을 말해봐라.
>     
> - **페이지 테이블 엔트리(PTE)** 안에 들어가는 주요 비트들
>     
>     (valid bit, protection bit, reference bit, modified(dirty) bit)을 하나씩 설명하고,
>     
>     각각이 **어떤 상황에서** 실제로 사용되는지 예를 들어 말해봐라.
>     
> - **요구 페이징(Demand Paging)** 과 **페이지 폴트(Page Fault)** 의 동작 흐름을,
>     
>     CPU 입장 → 하드웨어 트랩 → OS 처리 → 다시 사용자 코드로 복귀하는 순서대로
>     
>     단계별로 설명해봐라.
>     
> - 페이지 교체 알고리즘 **FIFO / Optimal / LRU / LFU / Clock** 를 떠올렸을 때,
>     
>     각각 **어떤 기준으로 희생 페이지를 고르는지** 한 줄씩 설명하고,
>     
>     특히 LFU가 **최근성(recency)을 반영하지 못해서 생기는 문제**를
>     
>     간단한 예시와 함께 설명해봐라.
>     
> - **스레싱(thrashing)** 이 무엇인지 정의하고,
>     
>     왜 “작업 집합(working set)을 담기엔 프레임이 모자랄 때” 잘 발생하는지 설명해봐라.
>     
>     그리고 프레임 할당 정책 **(균등 할당 / 비례 할당 / 작업 집합 / 페이지 폴트 빈도)** 이
>     
>     스레싱을 줄이기 위해 각각 어떤 아이디어를 쓰는지 큰 틀에서 비교해봐라.
>     

---

## 메모리 관리 개요

### 논리 주소와 물리 주소, 주소 바인딩

- **논리 주소(Logical Address)**
    - 각 **프로세스마다 독립적으로 가지는 주소 공간**이다.
    - 프로세스 입장에서는 항상 **0번지부터 시작하는 연속적인 공간**처럼 보인다.
    - **CPU가 명령어를 실행하면서 사용하는 주소**가 논리 주소이다.
- **물리 주소(Physical Address)**
    - **실제 DRAM(물리 메모리)** 상의 주소이다.
    - 여러 프로세스의 코드/데이터/스택이 섞여서 올라가 있다.

논리 주소 → 물리 주소로 바꾸는 작업을 **주소 바인딩(address binding)** 이라고 한다.

실제 실행 시에는 **MMU(메모리 관리 장치, Memory Management Unit)** 가

CPU가 만든 논리 주소를 보고 물리 주소로 변환한다.

- **CPU + MMU가 런타임에 변환을 수행**하고,
- **운영체제(OS)는 페이지 테이블, 세그먼트 테이블, 베이스/리미트 레지스터** 등을 설정함으로써
    
    변환 규칙을 관리한다.
    

### 연속 / 불연속 메모리 할당

프로세스의 주소 공간을 **물리 메모리 상에 어떻게 배치하는가**에 따라:

- **연속 할당(Contiguous Allocation)**
    - 하나의 프로세스에 할당되는 물리 메모리 공간이 **한 덩어리(연속 구간)** 로 배치된다.
    - 예:
        - 고정 분할(Fixed Partition)
        - 가변 분할(Variable Partition)
- **불연속 할당(Noncontiguous Allocation)**
    - 프로세스의 논리 주소 공간이 **여러 조각으로 나뉘어**
        
        물리 메모리 여기저기에 배치될 수 있다.
        
    - 예:
        - 페이징(Paging)
        - 세그멘테이션(Segmentation)
        - 페이징 + 세그멘테이션(Paged Segmentation)

---

## 연속 메모리 할당

연속 메모리 할당은 **하나의 프로세스가 하나의 연속된 물리 메모리 구간**을 차지하는 방식이다.

### 스와핑(Swapping)

메모리에 올라와 있는 프로세스들 중에는 **지금 당장 CPU를 사용하지 않는 프로세스**도 있다.

이럴 때:

- 그런 프로세스를 통째로 **보조기억장치(디스크)의 스왑 영역(swap area)** 으로 내보내고,
- 그 공간에 **다른 프로세스를 적재**하여 실행하는 방식을 **스와핑(swap)** 이라고 한다.

용어:

- **스왑 영역(Swap Area)**
    - 디스크의 일부로, **메모리에서 밀려난 프로세스(또는 페이지)** 를 저장하는 공간이다.
- **스왑 아웃(Swap-out)**
    - 프로세스를 메모리 → 스왑 영역으로 내보내는 것.
- **스왑 인(Swap-in)**
    - 스왑 영역에 있던 프로세스를 다시 메모리로 가져오는 것.

### 연속 할당에서의 메모리 할당 방식

메모리 내에 **여러 개의 빈 구간(홀, hole)** 이 있을 때,

어떤 빈 구간에 프로세스를 넣을지 결정하는 방식:

- **최초 적합(First Fit)**
    - 메모리의 앞에서부터 순서대로 검색하다가,
    - **처음으로 크기가 충분한 빈 공간**을 찾자마자 그곳에 할당.
    - 장점:
        - 검색을 많이 하지 않아도 되어 **빠른 할당**이 가능하다.
- **최적 적합(Best Fit)**
    - 모든 빈 공간을 다 검사한 뒤,
    - 프로세스가 들어갈 수 있는 **가장 작은 빈 공간**에 할당.
    - 장점:
        - 남는 공간(쓸데없이 큰 hole)을 줄이려는 시도.
    - 단점:
        - 작은 자투리 hole이 많이 생길 수 있다.
- **최악 적합(Worst Fit)**
    - 모든 빈 공간을 다 검사한 뒤,
    - **가장 큰 빈 공간**에 할당.
    - 의도:
        - 큰 공간을 잘게 쪼개서, 나머지 공간도 쓸 수 있게 해 보려는 전략.

### 외부 단편화(External Fragmentation)와 압축(Compaction)

연속 할당의 핵심 문제:

> 프로세스를 실행·종료하는 과정을 반복하면
> 
> 
> **프로세스 사이사이에 작은 빈 공간들이 흩어져** 생긴다.
> 
- 이 빈 공간들을 다 합치면 꽤 큰 공간이 될 수 있지만,
- **각각은 너무 작아서** 새로운 큰 프로세스를 담을 수 없다.
- 이런 **메모리 낭비 현상**을 **외부 단편화(external fragmentation)** 라고 한다.

**해결책 → 압축(Compaction)**

- 메모리 내에 흩어진 프로세스들을 **앞쪽으로 몰아서 붙이고**,
    
    빈 공간들을 **뒤쪽 한 덩어리**로 모으는 방식이다.
    
- 단점:
    - 메모리 내 데이터를 **대량으로 옮겨야 하므로 오버헤드**가 크다.
    - 압축하는 동안 **시스템 동작을 잠시 멈추거나, 매우 제한적으로만 동작**시켜야 한다.

**다른 해결책 → 가상 메모리 + 페이징**

- **프로세스를 일정한 단위(페이지)로 잘라서**,
    
    **물리 메모리 여기저기에 분산 배치**하면
    
    - “연속 구간”을 요구하지 않게 되고,
    - 외부 단편화 문제는 사실상 사라진다.
- 대신 “페이지 크기 단위로 남는 공간” → **내부 단편화**가 생길 수 있다

---

## 페이징과 가상 메모리

### 가상 메모리 개념

연속 할당의 두 가지 문제:

1. **외부 단편화**
2. **물리 메모리 크기보다 큰 프로세스를 실행할 수 없다**

**가상 메모리(Virtual Memory)** 의 핵심 아이디어:

> 프로그램 전체를 한 번에 메모리에 올리지 않고,
> 
> 
> **실제로 필요한 부분만 조금씩 메모리에 올려서**
> 
> 물리 메모리보다 훨씬 큰 프로세스도 실행 가능하게 하는 기술.
> 

가상 메모리 구현 방식의 대표 예:

- 페이징(Paging)
- 세그멘테이션(Segmentation)
- 페이징 + 세그멘테이션

### 페이징(Paging)이란?

외부 단편화의 근본 원인은:

> 크기가 제각각인 프로세스를
> 
> 
> **연속 구간**으로 메모리에 할당하려고 해서이다.
> 

**페이징 페러다임:**

- 프로세스의 논리 주소 공간을 **동일한 크기의 블록**인 **페이지(Page)** 로 자른다.
- 물리 메모리도 **페이지와 같은 크기**의 블록인 **프레임(Frame)** 으로 자른다.
- 논리 주소 공간의 페이지들을 **어떤 프레임에든 불연속적으로** 매핑해서 사용한다.

특징:

- 프로세스는 **논리 주소 공간에서는 연속**이지만,
- 실제 물리 메모리에서는 **페이지 단위로 여기저기 흩어져** 있을 수 있다.
- 외부 단편화 문제는 사라지고, 페이지 크기 단위의 **내부 단편화**만 남는다.

> 페이징을 사용하는 가상 메모리 시스템에서는
> 
> 
> 페이지 단위로 스왑 인/아웃이 일어난다.
> 
> 실행에 필요 없는 페이지는 디스크로 내리고(page-out),
> 
> 필요해지면 다시 메모리로 올린다(page-in).
> 

### 페이지 테이블 & TLB

프로세스의 페이지들이 물리 메모리에서 **불연속적으로** 배치되면,

CPU 입장에서 “논리 주소 → 어느 프레임 + 그 안의 위치인가?”를 알아야 한다.

이를 위해 **페이지 테이블(Page Table)** 을 사용한다.

- 페이지 테이블의 역할:
    - **페이지 번호 → 프레임 번호** 매핑 정보를 저장.
    - “이 프로세스의 i번째 페이지는 메모리의 몇 번째 프레임에 있는가?” 를 알려준다.
- 각 프로세스마다 **자기 전용 페이지 테이블**을 가진다.
    - 이 페이지 테이블 자체도 메모리에 저장된다.

문제:

- 단순하게 구현하면,
    1. 페이지 테이블을 읽기 위해 메모리 접근 1번,
    2. 실제 데이터/코드가 있는 프레임에 접근하기 위해 메모리 접근 1번
        
        → **매번 2번의 메모리 접근**이 필요해진다.
        

해결: **TLB(Translation Lookaside Buffer)**

- **페이지 테이블 엔트리의 캐시**를 CPU 근처(일반적으로 MMU 안)에 둔 것.
- 최근에 사용한 페이지 번호 → 프레임 번호 매핑을 TLB에 저장해두면,
    - TLB에서 바로 히트(hit)하는 경우 **추가 메모리 접근 없이** 바로 프레임 주소를 얻을 수 있다.
- TLB 미스(miss)일 때만 실제 페이지 테이블을 메모리에서 읽는다.

### 다단계 페이지 테이블 (Two-level, Multi-level)

- 논리 주소 공간이 크면 페이지 테이블 자체가 매우 커진다.
- 이 전체를 항상 메모리에 올려두는 것은 비효율적이다.
- 해결:
    - 페이지 테이블을 **여러 단계로 계층화**하여,
    - 필요한 부분만 메모리에 적재하는 구조를 사용한다.
    - 예: **Two-level page table**
        - 상위 페이지 테이블에서 **하위 페이지 테이블의 시작 주소**를 가리킴.
        - 사용 중인 주소 영역 주변의 하위 테이블만 메모리에 존재하면 되므로 메모리 절약.

### 공유 페이지(Shared Pages)

- 여러 프로세스가 **같은 코드(예: C 라이브러리, 공유 라이브러리)** 를 사용할 때,
    - 각자 코드 내용을 따로 복사하지 않고,
    - **같은 물리 프레임**을 가리키도록 페이지 테이블만 다르게 설정할 수 있다.
- 장점:
    - 코드(특히 re-entrant code)는 **한 번만 메모리에 올리고 여러 프로세스가 공유**할 수 있어 메모리 절약.

### 페이징에서의 주소 변환

페이징 시스템에서 논리 주소는 보통:

> 페이지 번호(Page Number) + 페이지 내 오프셋(Offset)
> 

으로 구성된다.

주소 변환 과정:

1. CPU가 논리 주소를 만든다: (페이지 번호 p, 오프셋 d)
2. MMU는 페이지 테이블에서 **p → 해당 프레임 번호 f** 를 찾는다.
3. 물리 주소 = (프레임 번호 f의 시작 주소) + 오프셋 d

여기서:

- 페이지/프레임은 **여러 바이트(예: 4KB)** 를 포함한다.
- 오프셋은 “프레임 시작 주소에서 몇 바이트 떨어져 있는가?”를 나타낸다.

### 페이지 테이블 엔트리(PTE; Page Table Entry)

페이지 테이블의 **각 행**을 **페이지 테이블 엔트리(PTE)** 라고 한다.

PTE에는 단순히 “페이지 번호 → 프레임 번호” 뿐 아니라

다음과 같은 플래그 비트들이 포함된다.

- **유효 비트(Valid bit)**
    - 이 페이지가 **현재 유효한지** 여부
    - 보통 의미:
        - 1: 이 페이지 엔트리가 **유효**하며, 실제 메모리(프레임)에 매핑되어 있다.
        - 0: 이 페이지는 현재 메모리에 없음(디스크에 있거나, 사용하지 않는 영역).
    - 유효 비트가 0인데 접근하면 → **페이지 폴트(page fault)** 발생.
- **보호 비트(Protection bits)**
    - 이 페이지에 대해 허용되는 접근 권한:
        - 읽기(Read), 쓰기(Write), 실행(Execute)
    - 예:
        - `100` → 읽기만 허용
        - `011` → 쓰기 + 실행 허용 (예시 표현)
- **참조 비트(Reference bit, Access bit)**
    - 이 페이지가 **최근에 접근된 적이 있는지** 나타낸다.
    - 페이지 교체 알고리즘(LRU 근사, Clock 알고리즘 등)에서
        
        “최근 사용 여부”를 판단하는 데 사용한다.
        
- **수정 비트(Modified bit, Dirty bit)**
    - 이 페이지에 **쓰기(write)가 일어난 적이 있는지** 여부
        - 1: 메모리 내에서 데이터가 변경됨 → 디스크 내용과 다름
        - 0: 변경된 적 없음 → 디스크 내용과 동일
    - 페이지를 쫓아낼 때:
        - Dirty bit = 1 → **디스크에 다시 써서 반영**해야 함.
        - Dirty bit = 0 → 디스크와 동일하므로 **그냥 버려도 됨** (쓰기 I/O 불필요).

---

## 페이지 교체와 프레임 할당

가상 메모리 덕분에 **물리 메모리보다 큰 프로세스**를 실행할 수 있지만,

물리 메모리 크기 자체는 여전히 한정되어 있다.

> 메모리 부족 시 어떤 페이지를 디스크로 내보낼지,
> 
> 
> 그리고 **프로세스마다 프레임을 얼마나 줄지**가 중요하다.
> 

### 요구 페이징(Demand Paging)과 페이지 폴트

**요구 페이징(Demand Paging)**

- 프로세스를 시작할 때 **모든 페이지를 한 번에 메모리에 올리지 않고**,
    
    **필요해지는 시점(첫 접근 시점)에 페이지를 올리는 기법**이다.
    
- 장점:
    - I/O 양 감소
    - 메모리 사용량 감소
    - 초기 응답 시간 단축
    - 같은 메모리로 더 많은 프로세스를 동시에 수용 가능

**페이지 폴트(Page Fault)**

- CPU가 어떤 페이지에 접근하려 할 때,
    - 해당 페이지의 유효 비트가 0이면 → **페이지가 메모리에 없음**
- 이때 하드웨어가 OS에게 **페이지 폴트 트랩**을 발생시키고,
- OS는 디스크에서 해당 페이지를 읽어와 빈 프레임에 적재한 뒤,
    - 페이지 테이블을 갱신하고
    - 해당 명령을 다시 실행한다.

이 시스템이 잘 동작하려면:

1. **페이지 교체 알고리즘** (어느 페이지를 내보낼 것인가)
2. **프레임 할당 방식** (각 프로세스에 프레임을 얼마나 줄 것인가)

이 두 가지가 중요하다.

### 페이지 교체 알고리즘

메모리가 가득 찬 상태에서 새 페이지를 올려야 하면,

**어느 페이지를 디스크로 내보낼지** 결정해야 한다.

→ 이 규칙이 **페이지 교체 알고리즘**이다.

일반적인 평가는:

> 페이지 폴트 횟수를 최소화하는 알고리즘일수록 “좋은” 알고리즘.
> 

### (1) FIFO 페이지 교체 (First-In First-Out)

- 메모리에 **가장 오래 전에 올라온 페이지부터** 쫓아내는 방식.
- 구현: 큐(Queue)
- 단점:
    - 오래 전에 올라왔지만 지금도 **매우 자주 사용하는 페이지**가 쫓겨날 수 있다.
    - **Belady’s anomaly**(프레임을 늘렸는데도 페이지 폴트가 증가하는 현상)가 발생할 수 있는 대표 알고리즘.

### (2) Optimal 페이지 교체 (OPT)

- “**가장 먼 미래에 참조될 페이지**를 교체하는 알고리즘”이다.
- CPU가 앞으로 **어떤 페이지를 언제 참조할지 모두 알고 있다**고 가정.
- 특징:
    - 이론적으로 **최소 페이지 폴트**를 보장하는 **이상적인 알고리즘**.
    - 그러나 현실에서 미래 참조를 알 수 없으므로 **직접 구현은 불가능**.
    - 다른 알고리즘의 성능을 비교할 때 **상한선 기준**으로 사용.

### (3) LRU 페이지 교체 (Least Recently Used)

- 이름 그대로 “**가장 오랫동안 사용되지 않은 페이지**를 교체”하는 알고리즘.
- 아이디어:
    - 과거에 최근에 많이 사용된 페이지는 앞으로도 사용할 가능성이 크고,
        
        오래 안 쓰인 페이지는 앞으로도 덜 쓸 것이라는 **지역성(locality)** 가정.
        
- 구현은:
    - 참조 시각을 저장하거나, 참조 순서를 관리하는 자료구조가 필요 → 완전한 LRU는 비용 큼.

### (4) LFU 페이지 교체 (Least Frequently Used)

- “**참조 횟수가 가장 적은 페이지**를 교체”하는 알고리즘.
- 장점:
    - 페이지의 **인기도(popularity)** 를 반영하려는 시도.
- 단점:
    - **최근성(recency)** 을 반영하지 못한다.
    - 예: 옛날에 많이 쓰였지만 지금은 안 쓰는 페이지가 계속 남아 있을 수 있다.

→ 실제 시스템에서 **순수 LRU/LFU를 정확하게 구현하는 것은 비용이 커서**

보통 근사 알고리즘(Clock 등)을 사용한다.

### (5) Clock 알고리즘 (Second-Chance 알고리즘)

LRU를 **하드웨어 지원 + 간단한 방식으로 근사**하는 알고리즘이다.

- 원형으로 연결된 페이지 프레임 리스트(시계 모양)를 두고,
    
    각 프레임마다 **Reference bit(참조 비트)** 를 둔다.
    
- “시계 바늘(clock hand)”이 프레임을 하나씩 가리키며 검사:

동작:

1. 현재 바늘이 가리키는 프레임의 **참조 비트가 0**이면:
    - “최근에 쓰인 적이 없다”는 의미 → **이 프레임을 교체 후보로 선택**.
2. 참조 비트가 **1**이면:
    - “최근에 쓰인 적이 있다”는 의미 →
        
        참조 비트를 **0으로 초기화하고**, 바늘을 다음 프레임으로 이동(두 번째 기회 부여).
        
3. 이 과정을 반복하면,
    
    결국 참조 비트가 0인 프레임을 만나 교체하게 된다.
    

→ **최근에 사용된 페이지는 한 번 더 기회를 주고,
오랫동안 안 쓰인 페이지를 자연스럽게 골라내는** LRU 근사 알고리즘이다.

### 스레싱(Thrashing)과 프레임 할당

**스레싱(Thrashing)**

- 프로세스가 실제로 **계산하는 시간보다**,
    
    **페이지를 올리고 내리는 시간(페이지 폴트 처리)** 이 더 많아서
    
    전체 시스템 성능이 급격히 나빠지는 현상이다.
    
- 원인:
    - 각 프로세스에 **너무 적은 프레임**만 할당해
        
        그 프로세스의 **작업 집합(실제로 자주 쓰는 페이지 집합)** 을 담지 못할 때 자주 발생.
        

→ 스레싱을 줄이기 위해 OS는 **프레임 할당 정책**을 잘 설계해야 한다.

### (1) 균등 할당(Equal Allocation)

- 모든 프로세스에 **동일한 개수의 프레임**을 나눠주는 방식.
- 단점:
    - 프로세스마다 크기/특성이 다른데도 **일괄적으로 동일 개수만 주므로 비합리적**이다.
    - 큰 프로세스는 프레임이 부족해 스레싱을 일으키기 쉽고,
        
        작은 프로세스는 프레임을 남길 수 있다.
        

### (2) 비례 할당(Proportional Allocation)

- 각 프로세스의 **주소 공간 크기(또는 우선순위)** 에 비례하여 프레임을 할당한다.
- 예:
    - 전체 메모리 프레임 수: 100
    - P1 주소 공간: 200KB, P2: 300KB, P3: 500KB
        
        → 각각 20, 30, 50 프레임처럼 비례해서 나눠줌.
        
- 장점:
    - 큰 프로세스에 더 많은 프레임을 줘서 **어느 정도 공정하게** 할당 가능.

### (3) 작업 집합(Working Set) 모델

- 아이디어:
    - 한 프로세스는 어느 순간에 **실제로 사용하는 페이지 집합**(Working Set)이 있다.
    - 이 작업 집합이 **메모리에 모두 올라와 있어야** 페이지 폴트가 적게 난다.
- OS는:
    - 각 프로세스의 최근 참조 페이지들을 추적해,
        
        **현재 작업 집합 크기**를 추정하고,
        
    - 그 크기만큼 프레임을 할당하려고 한다.
- 만약 전체 프로세스들의 작업 집합 합이 **물리 메모리보다 크면**:
    - 일부 프로세스를 통째로 **스왑 아웃**해서
    - 나머지 프로세스들이 충분한 프레임을 받도록 조정한다.
- 목표:
    - **스레싱이 시작되기 전에** 프레임을 늘리거나 프로세스를 줄여서 방지.

### (4) 페이지 폴트 빈도(Page-Fault Frequency, PFF) 방식

- 프로세스마다 **페이지 폴트 비율(page fault rate)** 을 모니터링한다.
- 정책:
    - 페이지 폴트 비율이 **너무 높으면**:
        - 해당 프로세스에 프레임을 **더 많이** 할당한다.
    - 페이지 폴트 비율이 **너무 낮으면**:
        - 프레임을 조금 **회수**해서 다른 프로세스에 줄 수 있다.
- 즉:
    - 페이지 폴트 비율을 **일정한 목표 범위 안**(상한/하한)으로 유지하도록
        
        프레임 개수를 동적으로 조절하는 방식이다.
